{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import zlib\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import sys\n",
    "\n",
    "compressed_sizes = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=0, usecols=[0, 1, 2, 3])  # Ignore extra columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary with pre-computed values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_compressed_sizes(user_ratings):\n",
    "    \n",
    "    for user, ratings in user_ratings.items():\n",
    "        u_string = \"\".join(f\"{k}:{v}\" for k, v in sorted(ratings.items()))\n",
    "        compressed_sizes[u_string] = len(zlib.compress(u_string.encode()))\n",
    "    \n",
    "    return compressed_sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_similarity(ratings_u, ratings_v):\n",
    "    common_items = set(ratings_u.keys()).intersection(set(ratings_v.keys()))\n",
    "    if not common_items:\n",
    "        return 0  # If no common items, similarity is 0\n",
    "\n",
    "    diff_sum = sum(abs(ratings_u[i] - ratings_v[i]) for i in common_items)\n",
    "    ls_value = 1 - (diff_sum / len(common_items))\n",
    "\n",
    "    return max(0, ls_value)  # Ensure similarity is non-negative\n",
    "\n",
    "# Compression Similarity with precomputed sizes\n",
    "def compression_similarity(ratings_u, ratings_v):\n",
    "    u_string = \"\".join(f\"{k}:{v}\" for k, v in sorted(ratings_u.items()))\n",
    "    v_string = \"\".join(f\"{k}:{v}\" for k, v in sorted(ratings_v.items()))\n",
    "    c_uv = len(zlib.compress((u_string + v_string).encode()))\n",
    "    c_u = compressed_sizes[u_string]\n",
    "    c_v = compressed_sizes[v_string]\n",
    "    return 1 - (c_uv - min(c_u, c_v)) / max(c_u, c_v)\n",
    "\n",
    "# Kolmogorov Similarity with precomputed sizes\n",
    "def kolmogorov_similarity(ratings_u, ratings_v):\n",
    "    u_string = \"\".join(f\"{k}:{v}\" for k, v in sorted(ratings_u.items()))\n",
    "    v_string = \"\".join(f\"{k}:{v}\" for k, v in sorted(ratings_v.items()))\n",
    "    c_u = compressed_sizes[u_string]\n",
    "    c_v = compressed_sizes[v_string]\n",
    "    return 1 / (1 + abs(c_u - c_v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simlarity Matrix and graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user similarity matrix and construct graph (with precomputed sizes)\n",
    "def compute_similarity_matrix(user_ratings, similarity_measure, compressed_sizes):\n",
    "    similarity_graph = nx.Graph()\n",
    "    for (u, v) in combinations(user_ratings.keys(), 2):\n",
    "        sim = similarity_measure(user_ratings[u], user_ratings[v])\n",
    "        if sim > 0.5:\n",
    "            \"EDGE\"\n",
    "            similarity_graph.add_edge(u, v, weight=sim)\n",
    "    return similarity_graph\n",
    "\n",
    "# Detect user clusters from similarity graph\n",
    "def detect_groups(similarity_graph):\n",
    "    return list(nx.connected_components(similarity_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reputation-based intra-clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute reputation-based ranking for each group\n",
    "def reputation_based_ranking(df, user_groups, lambda_factor=0.3, tol=1e-6, max_iter=10):\n",
    "    item_rankings = {item: 0.5 for item in df[\"item_id\"].unique()}  # Default starting ranking\n",
    "    user_reputation = {user: 1.0 for user in df[\"user_id\"].unique()}\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        print(\"Iteration\", iteration)\n",
    "        prev_rankings = item_rankings.copy()\n",
    "\n",
    "        # Compute rankings per cluster\n",
    "        cluster_rankings = {}\n",
    "        for group in user_groups:\n",
    "            group_users = list(group)\n",
    "            group_df = df[df[\"user_id\"].isin(group_users)]\n",
    "\n",
    "            for item, group_item in group_df.groupby(\"item_id\"):\n",
    "                users = group_item[\"user_id\"].values\n",
    "                weighted_sum = sum(user_reputation[u] * r for u, r in zip(users, group_item[\"normalized_rating\"].values))\n",
    "                total_weight = sum(user_reputation[u] for u in users)\n",
    "                cluster_rankings.setdefault(item, {})[frozenset(group)] = weighted_sum / total_weight if total_weight > 0 else None\n",
    "\n",
    "        # Assign rankings: if a product is unrated in a cluster, inherit from nearest cluster\n",
    "        for item in item_rankings.keys():\n",
    "            assigned = False\n",
    "            for group in user_groups:\n",
    "                if frozenset(group) in cluster_rankings.get(item, {}):\n",
    "                    item_rankings[item] = cluster_rankings[item][frozenset(group)]\n",
    "                    assigned = True\n",
    "                    break\n",
    "            if not assigned:  # Inherit from the closest cluster with a ranking\n",
    "                for other_item in cluster_rankings:\n",
    "                    for cluster in cluster_rankings[other_item]:\n",
    "                        if cluster_rankings[other_item][cluster] is not None:\n",
    "                            item_rankings[item] = cluster_rankings[other_item][cluster]\n",
    "                            break\n",
    "        # Update user reputations within each cluster\n",
    "        for user in df[\"user_id\"].unique():\n",
    "            user_ratings = df[df[\"user_id\"] == user]\n",
    "            items_rated = user_ratings[\"item_id\"].values\n",
    "\n",
    "            if len(items_rated) == 0:\n",
    "                continue\n",
    "\n",
    "            rating_errors = [abs(r - item_rankings[i]) for i, r in zip(items_rated, user_ratings[\"normalized_rating\"].values)]\n",
    "            avg_error = sum(rating_errors) / len(rating_errors)\n",
    "            \n",
    "            user_reputation[user] = max(1 - lambda_factor * avg_error, 0)\n",
    "\n",
    "        print(\"Iteration\", iteration, \"done\")\n",
    "        \n",
    "        # Check for convergence\n",
    "        ranking_diff = sum(abs(prev_rankings[i] - item_rankings[i]) for i in item_rankings if prev_rankings[i] is not None)\n",
    "        if iteration > 0 and ranking_diff < tol:\n",
    "            break\n",
    "\n",
    "    return item_rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comprimido\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Main execution\n",
    "file_path = \"/home/martim/Desktop/tese/datasets/book_crossing/book_ratings_normalized.dat\"\n",
    "df = load_dataset(file_path)\n",
    "\n",
    "# Prepare user ratings\n",
    "user_ratings = {user: dict(zip(group[\"item_id\"], group[\"normalized_rating\"])) for user, group in df.groupby(\"user_id\")}\n",
    "\n",
    "# Compute or load compressed sizes\n",
    "compressed_sizes = compute_compressed_sizes(user_ratings)\n",
    "print(\"comprimido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User clusters: 0\n",
       "Iteration 0\n",
       "Iteration 0 done\n",
       "Iteration 1\n",
       "Iteration 1 done\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose similarity measure: compression_similarity or kolmogorov_similarity\n",
    "similarity_graph = compute_similarity_matrix(user_ratings, compression_similarity, compressed_sizes)\n",
    "\n",
    "# Detect user clusters from similarity graph\n",
    "user_groups = list(nx.connected_components(similarity_graph))\n",
    "\n",
    "# Print the number of detected clusters\n",
    "print(f\"User clusters: {len(user_groups)}\")\n",
    "\n",
    "# Compute rankings\n",
    "rankings = reputation_based_ranking(df, user_groups)\n",
    "\n",
    "# Extract rankings\n",
    "ratings = list(rankings.values())\n",
    "\n",
    "# Plot distribution\n",
    "bins = np.arange(0.1, 1.1, 0.1)\n",
    "hist, bin_edges = np.histogram(ratings, bins=bins)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bin_edges[:-1], hist, width=0.08, align='edge', edgecolor='black')\n",
    "\n",
    "# Add counts on top of bars\n",
    "for i in range(len(hist)):\n",
    "    plt.text(bin_edges[i], hist[i] + 1, str(hist[i]), ha='center', fontsize=12)\n",
    "\n",
    "plt.xlabel('Rating', fontsize=12)\n",
    "plt.ylabel('Number of Books', fontsize=12)\n",
    "plt.title('Multipartite Reputation-Based Book Rankings', fontsize=14)\n",
    "plt.xticks(bins)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
